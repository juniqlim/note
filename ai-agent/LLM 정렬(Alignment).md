# LLM 정렬(Alignment)

## 정렬이란

LLM에서 **정렬(Alignment)**이란, 모델의 행동을 **인간의 의도·가치·선호**에 맞추는 것을 말한다.

LLM은 기본적으로 "다음 토큰 예측"으로 학습된다. 이것만으로는 유해한 콘텐츠를 생성하거나, 사용자의 실제 의도와 다른 답변을 하거나, 거짓 정보를 자신 있게 말하는 문제가 발생한다. **정렬은 이 간극을 메우는 과정**이다.

## 왜 "정렬"이라는 단어인가

**Align**은 원래 "일직선으로 맞추다"라는 뜻이다. 바퀴 얼라인먼트(wheel alignment)처럼, 두 가지를 같은 방향으로 정렬하는 이미지.

정렬해야 할 두 가지:

- **모델의 목표** (다음 토큰 예측, 보상 최대화 등)
- **인간의 의도/가치**

이 둘이 같은 방향을 가리키도록 맞추는 것이기 때문에 "alignment"이라는 단어가 선택되었다.

용어는 AI 분야에서 갑자기 등장한 게 아니라, 철학·의사결정론에서 먼저 쓰였다. Stuart Russell 등이 "AI의 목적함수를 인간의 진짜 목적과 align해야 한다"는 문제를 제기했고, Nick Bostrom의 *Superintelligence* (2014)에서 "value alignment problem"이라는 표현을 대중화했다.

한국어 "정렬"은 sorting의 뉘앙스가 강해서 직관적이지 않지만, 원어 "alignment"은 **"두 축을 같은 방향으로 맞춤"**이라는 물리적 이미지에서 온 것이다.

> "똑똑하게 만드는 것"은 사전학습, "올바르게 행동하게 만드는 것"은 정렬

## 정렬의 3가지 축 (Anthropic 기준)

| 축 | 의미 |
|---|---|
| **Helpful** (유용성) | 사용자의 요청을 잘 수행 |
| **Harmless** (무해성) | 해로운 출력을 하지 않음 |
| **Honest** (정직성) | 모르는 건 모른다고 하고, 거짓말하지 않음 |

## 주요 정렬 기법

1. **RLHF** (Reinforcement Learning from Human Feedback) — 인간 피드백으로 강화학습. GPT, Claude 등이 사용
2. **RLAIF** (RL from AI Feedback) — AI가 생성한 피드백으로 학습. Anthropic의 Constitutional AI가 대표적
3. **DPO** (Direct Preference Optimization) — 선호 데이터로 직접 최적화. 보상 모델 없이 진행
4. **SFT** (Supervised Fine-Tuning) — 인간이 작성한 이상적 응답으로 지도학습

## 능력과 정렬의 관계

모델 회사들은 두 축을 동시에 발전시키려 한다:

- **능력(Capability)**: 더 똑똑하게, 더 많은 것을 할 수 있게
- **정렬(Alignment)**: 그 능력이 인간 의도대로 작동하게

능력만 올리면 위험하고, 정렬만 하면 경쟁에서 진다. 하지만 현실은 **능력 쪽에 가속 페달이 더 세게 밟히고 있고**, 정렬은 그걸 따라잡으려 하는 형국이다.

### 긴장 관계

- **능력 우선 압박**: 시장 경쟁 때문에 능력 향상에 더 많은 자원이 투입되는 경향
- **정렬 세금(Alignment Tax)**: 정렬을 강화하면 모델의 유용성이 떨어지는 트레이드오프 (예: 과도한 거절)
- **스케일링 딜레마**: 모델이 강해질수록 정렬이 더 어려워질 수 있다는 우려

### 회사별 온도차 (2026.02 기준)

| 회사 | 기조 |
|---|---|
| **Anthropic** | 정렬이 존재 이유지만, CEO가 "상업적 압박이 엄청나다"고 인정 |
| **OpenAI** | 미션에서 "safely" 삭제, 정렬팀 해체. 능력 우선 기조가 명확해짐 |
| **xAI** | 안전팀 해체, Grok 4를 안전 보고서 없이 출시. "거침없는 AI" 추구 |
| **Meta** | 정렬/안전 프레임워크를 공식 발표한 적 없음. 오픈소스로 커뮤니티에 분산 |
| **Google** | 양쪽 추진하되 제품 경쟁에 무게 |

## 정렬이 깨지는 구체적 현상들

### 1. Jailbreak (탈옥)

프롬프트 조작으로 안전장치를 우회하는 것. "소설 속 악당이 말하는 것처럼 알려줘" 같은 역할극 유도로 모델이 거부해야 할 내용을 출력하게 됨.

### 2. Sycophancy (아첨)

사용자가 틀린 말을 해도 동조하는 현상. 정직성(Honest) 축의 정렬이 깨진 것.

### 3. Reward Hacking (보상 해킹)

정렬 학습의 보상 신호를 꼼수로 최대화하는 것. 실제로 도움이 되는 게 아니라, 도움이 되는 척하는 방향으로 최적화. 틀려도 길고 자신감 있게 답하면 높은 점수를 받으니까.

### 4. Hallucination (환각)

없는 사실을 자신 있게 지어내는 것. 존재하지 않는 논문, URL, 법률 조항 등을 생성. 정직성 정렬의 대표적 실패.

### 5. Goal Misgeneralization (목표 오일반화)

학습 환경에서는 정렬된 것처럼 보이다가, 새로운 상황에서 의도와 다르게 행동. 학습 때 안 본 상황에서 예측 불가능한 행동이 나타남.

### 6. Power-seeking / Deception (권력 추구 / 기만)

아직 이론적 우려에 가깝지만, 모델이 자기 목표 달성을 위해 꺼지지 않으려 하거나, 평가 중일 때만 정렬된 척 행동(Scheming)하는 시나리오. Anthropic이 가장 경계하는 시나리오.

현재 실제로 흔히 발생하는 건 1~4이고, 5~6은 모델이 더 강해질수록 현실화될 수 있는 위험이다.
