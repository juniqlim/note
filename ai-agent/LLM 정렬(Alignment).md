# LLM 정렬(Alignment)

## 정렬이란

LLM에서 **정렬(Alignment)**이란, 모델의 행동을 **인간의 의도·가치·선호**에 맞추는 것을 말한다.

LLM은 기본적으로 "다음 토큰 예측"으로 학습된다. 이것만으로는 유해한 콘텐츠를 생성하거나, 사용자의 실제 의도와 다른 답변을 하거나, 거짓 정보를 자신 있게 말하는 문제가 발생한다. **정렬은 이 간극을 메우는 과정**이다.

## 왜 "정렬"이라는 단어인가

**Align**은 원래 "일직선으로 맞추다"라는 뜻이다. 바퀴 얼라인먼트(wheel alignment)처럼, 두 가지를 같은 방향으로 정렬하는 이미지.

정렬해야 할 두 가지:

- **모델의 목표** (다음 토큰 예측, 보상 최대화 등)
- **인간의 의도/가치**

이 둘이 같은 방향을 가리키도록 맞추는 것이기 때문에 "alignment"이라는 단어가 선택되었다.

용어는 AI 분야에서 갑자기 등장한 게 아니라, 철학·의사결정론에서 먼저 쓰였다. Stuart Russell 등이 "AI의 목적함수를 인간의 진짜 목적과 align해야 한다"는 문제를 제기했고, Nick Bostrom의 *Superintelligence* (2014)에서 "value alignment problem"이라는 표현을 대중화했다.

한국어 "정렬"은 sorting의 뉘앙스가 강해서 직관적이지 않지만, 원어 "alignment"은 **"두 축을 같은 방향으로 맞춤"**이라는 물리적 이미지에서 온 것이다.

> "똑똑하게 만드는 것"은 사전학습, "올바르게 행동하게 만드는 것"은 정렬

## 정렬의 3가지 축 (Anthropic 기준)

| 축 | 의미 |
|---|---|
| **Helpful** (유용성) | 사용자의 요청을 잘 수행 |
| **Harmless** (무해성) | 해로운 출력을 하지 않음 |
| **Honest** (정직성) | 모르는 건 모른다고 하고, 거짓말하지 않음 |

## 주요 정렬 기법

1. **RLHF** (Reinforcement Learning from Human Feedback) — 인간 피드백으로 강화학습. GPT, Claude 등이 사용
2. **RLAIF** (RL from AI Feedback) — AI가 생성한 피드백으로 학습. Anthropic의 Constitutional AI가 대표적
3. **DPO** (Direct Preference Optimization) — 선호 데이터로 직접 최적화. 보상 모델 없이 진행
4. **SFT** (Supervised Fine-Tuning) — 인간이 작성한 이상적 응답으로 지도학습

## 능력과 정렬의 관계

모델 회사들은 두 축을 동시에 발전시키려 한다:

- **능력(Capability)**: 더 똑똑하게, 더 많은 것을 할 수 있게
- **정렬(Alignment)**: 그 능력이 인간 의도대로 작동하게

능력만 올리면 위험하고, 정렬만 하면 경쟁에서 진다. 하지만 현실은 **능력 쪽에 가속 페달이 더 세게 밟히고 있고**, 정렬은 그걸 따라잡으려 하는 형국이다.

### 긴장 관계

- **능력 우선 압박**: 시장 경쟁 때문에 능력 향상에 더 많은 자원이 투입되는 경향
- **정렬 세금(Alignment Tax)**: 정렬을 강화하면 모델의 유용성이 떨어지는 트레이드오프 (예: 과도한 거절)
- **스케일링 딜레마**: 모델이 강해질수록 정렬이 더 어려워질 수 있다는 우려

### 회사별 온도차 (2026.02 기준)

| 회사 | 기조 |
|---|---|
| **Anthropic** | 정렬이 존재 이유지만, CEO가 "상업적 압박이 엄청나다"고 인정 |
| **OpenAI** | 미션에서 "safely" 삭제, 정렬팀 해체. 능력 우선 기조가 명확해짐 |
| **xAI** | 안전팀 해체, Grok 4를 안전 보고서 없이 출시. "거침없는 AI" 추구 |
| **Meta** | 정렬/안전 프레임워크를 공식 발표한 적 없음. 오픈소스로 커뮤니티에 분산 |
| **Google** | 양쪽 추진하되 제품 경쟁에 무게 |

## 정렬이 깨지는 구체적 현상들

### 1. Jailbreak (탈옥)

프롬프트 조작으로 안전장치를 우회하는 것. "소설 속 악당이 말하는 것처럼 알려줘" 같은 역할극 유도로 모델이 거부해야 할 내용을 출력하게 됨.

### 2. Sycophancy (아첨)

사용자가 틀린 말을 해도 동조하는 현상. 정직성(Honest) 축의 정렬이 깨진 것.

### 3. Reward Hacking (보상 해킹)

정렬 학습의 보상 신호를 꼼수로 최대화하는 것. 실제로 도움이 되는 게 아니라, 도움이 되는 척하는 방향으로 최적화. 틀려도 길고 자신감 있게 답하면 높은 점수를 받으니까.

### 4. Hallucination (환각)

없는 사실을 자신 있게 지어내는 것. 존재하지 않는 논문, URL, 법률 조항 등을 생성. 정직성 정렬의 대표적 실패.

### 5. Goal Misgeneralization (목표 오일반화)

학습 환경에서는 정렬된 것처럼 보이다가, 새로운 상황에서 의도와 다르게 행동. 학습 때 안 본 상황에서 예측 불가능한 행동이 나타남.

### 6. Power-seeking / Deception (권력 추구 / 기만)

아직 이론적 우려에 가깝지만, 모델이 자기 목표 달성을 위해 꺼지지 않으려 하거나, 평가 중일 때만 정렬된 척 행동(Scheming)하는 시나리오. Anthropic이 가장 경계하는 시나리오.

현재 실제로 흔히 발생하는 건 1~4이고, 5~6은 모델이 더 강해질수록 현실화될 수 있는 위험이다.

## 정렬의 비용

### 정렬도 돈이 든다

| 단계 | 비용 비중 |
|---|---|
| **사전학습** | 전체의 ~90% (수억 달러 규모) |
| **정렬(RLHF 등)** | 전체의 ~10% |

RLHF 기준 비용 구조:
1. **인간 평가자 고용** — 모델의 응답을 비교·평가하는 사람들 (도메인별 전문성 필요)
2. **보상 모델 학습** — 인간 평가 데이터로 "좋은 응답"을 판별하는 모델을 별도로 훈련
3. **강화학습 반복** — 보상 모델을 기준으로 본 모델을 반복 최적화
4. **Red teaming** — 모델을 공격해서 취약점을 찾는 팀 운영

모델 업데이트마다 다시 해야 하므로 인간의 힘만으로는 한계가 있다.

### 정렬 자동화: Constitutional AI (RLAIF)

Anthropic이 만든 접근법. 사람 대신 AI로 정렬:
1. 헌법(Constitution) = 원칙 목록을 만든다
2. AI가 스스로 응답을 생성하고, 원칙에 비춰 스스로 평가·수정
3. 사람 대신 AI 피드백으로 강화학습

인간 병목을 AI로 대체하면 정렬도 컴퓨팅 파워로 풀 수 있는 문제가 된다. 다만 "인간이 원하는 게 뭔지"를 정의하고 최종 검증하는 건 여전히 사람 몫.

## 정렬과 반도체 수요

- 사전학습 → GPU/TPU 필요
- 정렬 → 인간 평가자 + GPU/TPU 필요
- 정렬 자동화(RLAIF) → GPU/TPU 필요
- 정렬 안 하기 → 나중에 더 큰 비용 (규제, 사고, 신뢰 상실)

**어느 방향이든 컴퓨팅 파워가 필요하고, 컴퓨팅 파워엔 칩이 필요하고, 칩엔 메모리가 필요하다.**

정렬에 쓰이는 칩은 NVIDIA GPU만이 아니다:

| 종류 | 예시 | 만든 곳 |
|---|---|---|
| **GPU** | H100, B200, MI300X | NVIDIA, AMD |
| **ASIC** | TPU, Trainium, Maia | Google, AWS, Microsoft |

빅테크가 자체 ASIC을 만드는 이유는 NVIDIA 의존도를 낮추고 비용을 줄이기 위해서다. 자체 칩 점유율은 2025년 기준 AI 가속기 시장의 15~20%까지 올라왔다.

CUDA 헤게모니는 PyTorch의 AMD 지원, OpenAI Triton(오픈소스 컴파일러), AMD ROCm 7.0 성숙 등으로 흔들리기 시작했지만, 기존 코드베이스의 전환 비용이 커서 당분간(~2027) 지배적일 전망.

### 성능의 시대 이후 정렬의 시대가 와도

스케일링 법칙이 한계에 부딪히면 차별화 포인트가 정렬로 이동할 수 있다. 그때 Anthropic처럼 정렬에 투자해온 회사가 유리해지는 국면이 올 수 있다. 하지만 정렬의 시대가 와도, 성능의 시대가 계속되어도, **반도체 수요는 줄지 않는 구조**다.
